# NLP_Final_Project

# GPT-small & Retrieval-Augmented Generation (RAG)

This repository contains the implementation of a lightweight Transformer-based language model (GPT-small) and a retrieval-augmented generation (RAG) system for open-domain question answering. The project was completed as the final assignment for CSE402 at UNIST.

---

## ğŸ“Œ Overview

The project consists of two main components:

1. **GPT-small**  
   - A decoder-only Transformer model built from scratch using PyTorch.
   - Includes modern architectural features:
     - Rotary Positional Embedding (RoPE)
     - Grouped Query Attention (GQA)
     - RMSNorm
   - Pretrained using causal language modeling on a curated version of the SmolLM-12.5 Cosmopedia corpus.
   - Fine-tuned on two downstream tasks:
     - **Text Summarization** (XSum)
     - **News Classification** (AG News)

2. **Retrieval-Augmented Generation (RAG)**  
   - Combines sparse retrieval (BM25 via Pyserini) with generative models.
   - Supports both:
     - GPT-small (fine-tuned)
     - LLaMA-3.2-1B-Instruct (zero-shot)
   - Enhanced using:
     - Prompt engineering (e.g., few-shot examples)
     - Answer extraction via post-processing (e.g., JSON parsing)

---

## ğŸ§  Model Details

- Architecture: Decoder-only Transformer
- Parameters: ~63.58M
- Pretraining objective: Causal Language Modeling (CLM)
- Optimizations:
  - RoPE for positional encoding
  - GQA for efficient attention
  - RMSNorm for stable training

---

## ğŸ“ Project Structure
```
.
â”œâ”€â”€ model/
â”‚   â”œâ”€â”€ model.py               # GPT-small model with RoPE, GQA, RMSNorm
â”‚   â””â”€â”€ rag.py                 # RAG model: retrieval + generation logic
â”œâ”€â”€ dataset/
â”‚   â”œâ”€â”€ summary.py             # XSum summarization preprocessing
â”‚   â”œâ”€â”€ classification.py      # AG News classification preprocessing
â”‚   â””â”€â”€ rag.py                 # NQ dataset loading for RAG
â”œâ”€â”€ cache/                     # Local dataset/model cache (auto-generated)
â”œâ”€â”€ main.ipynb                 # Main notebook for pretraining, finetuning, RAG
â”œâ”€â”€ README.md                  # This file
â””â”€â”€ requirements.txt           # Dependencies (e.g., torch, transformers, pyserini)
```

## ğŸš€ How to Run

### 1. Environment Setup

Install dependencies:
```bash
pip install -r requirements.txt
2. Run Each Stage
ëª¨ë“  ì‹¤í–‰ì€ main.ipynbì—ì„œ í”Œë˜ê·¸ë§Œ ì„¤ì •í•˜ë©´ ë©ë‹ˆë‹¤.

âœ… Pretraining GPT-small
DO_PRETRAIN = True
DO_FINETUNE_SM = False
DO_FINETUNE_CF = False
DO_FINETUNE_RAG = False
DO_ZEROSHOT_RAG = False
DO_SUBMISSION = False

âœ… Fine-tuning on Summarization
DO_PRETRAIN = False
DO_FINETUNE_SM = True
DO_FINETUNE_CF = False
DO_FINETUNE_RAG = False
DO_ZEROSHOT_RAG = False
DO_SUBMISSION = False

âœ… Fine-tuning on Classification
DO_PRETRAIN = False
DO_FINETUNE_SM = False
DO_FINETUNE_CF = True
DO_FINETUNE_RAG = False
DO_ZEROSHOT_RAG = False
DO_SUBMISSION = False

âœ… RAG Fine-tuning with GPT-small
DO_PRETRAIN = False
DO_FINETUNE_SM = False
DO_FINETUNE_CF = False
DO_FINETUNE_RAG = True
DO_ZEROSHOT_RAG = False
DO_SUBMISSION = False

âœ… Zero-shot RAG with LLaMA-3.2-1B-Instruct
DO_PRETRAIN = False
DO_FINETUNE_SM = False
DO_FINETUNE_CF = False
DO_FINETUNE_RAG = False
DO_ZEROSHOT_RAG = True
DO_SUBMISSION = False

âœ… Final Submission Package
DO_PRETRAIN = False
DO_FINETUNE_SM = False
DO_FINETUNE_CF = False
DO_FINETUNE_RAG = False
DO_ZEROSHOT_RAG = False
DO_SUBMISSION = True
```

## ğŸ“¦ Outputs

defaultproject_report.pdf â€” Final report (6~8 pages, ACL style)
defaultproject_codes.zip â€” All code + README
defaultproject_supplementaries.zip â€” Pretrained + fine-tuned model weights
Please ensure your code runs successfully in main.ipynb.

## ğŸ“š Reference Papers

Transformer: Vaswani et al., 2017
RoPE: Su et al., 2023
GQA: Ainslie et al., 2023
RMSNorm: Zhang & Sennrich, 2019
RAG: Lewis et al., 2020
Self-RAG: Asai et al., 2024
PCW: Ratner et al., 2023
CoT Prompting: Wei et al., 2022
